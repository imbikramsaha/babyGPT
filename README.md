# babyGPT
# BabyGPT

BabyGPT is a small, experimental language model with 19 million parameters. It's inspired by Andrej @Karpathy 's minGPT and nanoGPT projects.  It's designed to be a starting point for understanding and experimenting with the core concepts of large language models, but at a scale that's easier to manage and train.

## Features

* **Small Size:** With only 19 million parameters, BabyGPT is significantly smaller than state-of-the-art models, making it more accessible for research and experimentation on limited hardware.
* **Transformer Architecture:** BabyGPT is based on the Transformer architecture, which is the foundation of many modern large language models.
* **Character-Level Generation:** The model is trained to generate text at the character level.
* **Basic Functionality:** Demonstrates core language modeling concepts.

## Upcoming: BabyGPT Bengali Version

We are excited to announce that a Bengali version of BabyGPT is coming soon! This version will be trained on Bengali text data and will be capable of generating Bengali text. Stay tuned for updates!

## Model Details

* **Architecture:** Transformer-based language model.
* **Parameters:** 19 million
* **Training Data:** The model is trained on character-level text data.  Currently, the repository includes the following datasets:
    * `micro-Tagore.txt`
    * `tiny-TagorePoems.txt`
    * `tiny-shakespeare.txt`

## Coming Soon: Bengali Version Details

* **Language:** Bengali
* **Training Data:** BabyGPT Bengali will be trained on `tiny-TagorePoems.txt` Bengali text.

